Bootstrap: docker
#From: nvidia/cuda:11.8.0-devel-ubuntu22.04
From: nvidia/cuda:12.4.1-devel-ubuntu22.04

%environment
    export PYTHONUNBUFFERED=1
    export DEBIAN_FRONTEND=noninteractive
    export TZ=Europe/Berlin
    export MAX_JOBS=4
%files
    requirements.txt /opt
%post
    if [ -f /opt/requirements.txt ]; then
            echo "requirements.txt found - yey"
    fi

    # Install system dependencies
    apt-get update && apt-get install -y \
        build-essential \
        cmake \
        curl \
        cython3 \
        gcc \
        git \
        libibverbs1 \
        libnl-3-dev \
        libnl-route-3-dev \
        libudev-dev \
        ninja-build \
        pandoc \
        pkg-config \
        python3-dev \
        python3-distutils \
        python3-docutils \
        python3-venv \
        valgrind \
        && rm -rf /var/lib/apt/lists/*

    git clone https://github.com/linux-rdma/rdma-core.git \
    && cd rdma-core
    bash build.sh
    cd .. && rm -rf rdma-core

    ln -sf /usr/bin/python3 /usr/bin/python

    # Install pip
    curl -sS https://bootstrap.pypa.io/get-pip.py | python

    # Install PyTorch Nightly
    python -m pip install ninja packaging
    python -m pip install --index-url https://download.pytorch.org/whl/cu124 torch
    python -m pip install -r /opt/requirements.txt --no-build-isolation
    # Clone and install Flash-Attention
    git clone https://github.com/Dao-AILab/flash-attention \
        && cd flash-attention \
        && cd csrc/rotary && python -m pip install . \
        && cd ../layer_norm && python -m pip install . \
        && cd ../xentropy && python -m pip install . \
        && cd ../.. \
        && cd hopper \
        && python setup.py install \
        && cd ../.. && rm -rf flash-attention

%runscript
    exec python3
